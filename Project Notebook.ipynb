{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions.split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karate_text: org.apache.spark.rdd.RDD[String] = data/soc-karate/soc-karate.mtx MapPartitionsRDD[440] at textFile at <console>:51\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val karate_text = sc.textFile(\"data/soc-karate/soc-karate.mtx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_karate: org.apache.spark.sql.DataFrame = [value: string, id: bigint]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_karate_text = karate_text.toDF.withColumn(\"id\",monotonicallyIncreasingId)\n",
    "df_karate_text = df_karate_text.withColumn(\"rank\", row_number().over(Window.orderBy(\"id\")))\n",
    "df_karate_text = df_karate_text.filter(df_karate(\"rank\")>24)\n",
    "df_karate_text = df_karate_text.drop(\"id\",\"rank\")\n",
    "\n",
    "val df_karate_splitted = df_karate.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_karate_splitted: org.apache.spark.sql.DataFrame = [srcId: string, dstID: string]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_karate_splitted = df_karate.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[19] at rdd at <console>:39\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rows: RDD[Row] = df_karate_splitted.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Array[org.apache.spark.sql.Row] = Array([2,1], [3,1], [4,1], [5,1], [6,1], [7,1], [8,1], [9,1], [11,1], [12,1], [13,1], [14,1], [18,1], [20,1], [22,1], [32,1], [3,2], [4,2], [8,2], [14,2], [18,2], [20,2], [22,2], [31,2], [4,3], [8,3], [9,3], [10,3], [14,3], [28,3], [29,3], [33,3], [8,4], [13,4], [14,4], [7,5], [11,5], [7,6], [11,6], [17,6], [17,7], [31,9], [33,9], [34,9], [34,10], [34,14], [33,15], [34,15], [33,16], [34,16], [33,19], [34,19], [34,20], [33,21], [34,21], [33,23], [34,23], [26,24], [28,24], [30,24], [33,24], [34,24], [26,25], [28,25], [32,25], [32,26], [30,27], [34,27], [34,28], [32,29], [34,29], [33,30], [34,30], [33,31], [34,31], [33,32], [34,32], [34,33])\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[418] at map at <console>:43\n",
       "default_user: (Int, Int) = (0,0)\n",
       "graph: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@16d9ba15\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var a = rows.map{ case Row(src:String, dist : String) => Edge(src.toLong, dist.toLong,1)}\n",
    "var default_user=(0, 0)\n",
    "var graph = Graph.fromEdges(a, default_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,1), Edge(3,1,1), Edge(3,2,1), Edge(4,1,1), Edge(4,2,1), Edge(4,3,1), Edge(5,1,1), Edge(6,1,1), Edge(7,1,1), Edge(7,5,1), Edge(7,6,1), Edge(8,1,1), Edge(8,2,1), Edge(8,3,1), Edge(8,4,1), Edge(9,1,1), Edge(9,3,1), Edge(10,3,1), Edge(11,1,1), Edge(11,5,1), Edge(11,6,1), Edge(12,1,1), Edge(13,1,1), Edge(13,4,1), Edge(14,1,1), Edge(14,2,1), Edge(14,3,1), Edge(14,4,1), Edge(17,6,1), Edge(17,7,1), Edge(18,1,1), Edge(18,2,1), Edge(20,1,1), Edge(20,2,1), Edge(22,1,1), Edge(22,2,1), Edge(26,24,1), Edge(26,25,1), Edge(28,3,1), Edge(28,24,1), Edge(28,25,1), Edge(29,3,1), Edge(30,24,1), Edge(30,27,1), Edge(31,2,1), Edge(31,9,1), Edge(32,1,1), Edge(32,25,1), Edge(32,26,1), Edge(32,29,1), Edge(33,3,1), Edge(33,9,1), Edge(33,15,1), Edge(33...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(org.apache.spark.graphx.VertexId, (Int, Int))] = Array((20,(0,0)), (13,(0,0)), (19,(0,0)), (34,(0,0)), (15,(0,0)), (4,(0,0)), (21,(0,0)), (16,(0,0)), (22,(0,0)), (25,(0,0)), (28,(0,0)), (29,(0,0)), (11,(0,0)), (14,(0,0)), (32,(0,0)), (30,(0,0)), (24,(0,0)), (27,(0,0)), (33,(0,0)), (23,(0,0)), (1,(0,0)), (6,(0,0)), (17,(0,0)), (3,(0,0)), (7,(0,0)), (9,(0,0)), (8,(0,0)), (12,(0,0)), (18,(0,0)), (31,(0,0)), (26,(0,0)), (10,(0,0)), (5,(0,0)), (2,(0,0)))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des degrés des noeuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newgraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@6abee079\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newgraph = graph.outerJoinVertices(graph.degrees)((index,_,deg) => (index,deg.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[org.apache.spark.graphx.EdgeTriplet[(org.apache.spark.graphx.VertexId, Int),Int]] = Array(((2,(2,9)),(1,(1,16)),1), ((3,(3,10)),(1,(1,16)),1), ((3,(3,10)),(2,(2,9)),1), ((4,(4,6)),(1,(1,16)),1), ((4,(4,6)),(2,(2,9)),1), ((4,(4,6)),(3,(3,10)),1), ((5,(5,3)),(1,(1,16)),1), ((6,(6,4)),(1,(1,16)),1), ((7,(7,4)),(1,(1,16)),1), ((7,(7,4)),(5,(5,3)),1), ((7,(7,4)),(6,(6,4)),1), ((8,(8,4)),(1,(1,16)),1), ((8,(8,4)),(2,(2,9)),1), ((8,(8,4)),(3,(3,10)),1), ((8,(8,4)),(4,(4,6)),1), ((9,(9,5)),(1,(1,16)),1), ((9,(9,5)),(3,(3,10)),1), ((10,(10,2)),(3,(3,10)),1), ((11,(11,3)),(1,(1,16)),1), ((11,(11,3)),(5,(5,3)),1), ((11,(11,3)),(6,(6,4)),1), ((12,(12,1)),(1,(1,16)),1), ((13,(13,2)),(1,(1,16)),1), ((13,(13,2)),(4,(4,6)),1), ((14,(14,5)),(1,(1,16)),1), ((14,(14,5)),(2,(2,9)),1), ((14,(14,...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgraph.triplets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(org.apache.spark.graphx.VertexId, (org.apache.spark.graphx.VertexId, Int))] = Array((20,(20,3)), (13,(13,2)), (19,(19,2)), (34,(34,17)), (15,(15,2)), (4,(4,6)), (21,(21,2)), (16,(16,2)), (22,(22,2)), (25,(25,3)), (28,(28,4)), (29,(29,3)), (11,(11,3)), (14,(14,5)), (32,(32,6)), (30,(30,4)), (24,(24,5)), (27,(27,2)), (33,(33,12)), (23,(23,2)), (1,(1,16)), (6,(6,4)), (17,(17,2)), (3,(3,10)), (7,(7,4)), (9,(9,5)), (8,(8,4)), (12,(12,1)), (18,(18,2)), (31,(31,4)), (26,(26,3)), (10,(10,2)), (5,(5,3)), (2,(2,9)))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgraph.vertices.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 1: Variables utiles pour calculer le gain en modularité lors de la suppression d'un noeud d'une communauté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaIn1: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "sigmaTot1: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmaIn1(i:Long, comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1==comId) &&\n",
    "                                                    !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def sigmaTot1(i:Long, comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId) && !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 2: Variables utiles pour calculer le gain en modularité lors de l'ajout d'un noeud à une communauté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaIn2: (comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "kiin2: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "sigmaTot2: (comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmaIn2(comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcAttr._1==comId && triplet.dstAttr._1==comId))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin2(i:Long, comId : Long, graph : Graph[(Long, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstAttr._1==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def sigmaTot2(comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables utiles au 2 étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ki: (i: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "kiin: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ki(i: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcId==i || triplet.dstId==i))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin(i:Long, comId : Long, graph : Graph[(Long, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstAttr._1==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du différentiel de modularité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQ: (sigmain: Double, sigmatot: Double, kivar: Double, kiinvar: Double, m: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DQ(sigmain:Double, sigmatot:Double, kivar:Double, kiinvar:Double, m:Long, graph:Graph[(Long, Int),Int]):Double = {\n",
    "    return (((sigmain+kiinvar)/(2*m) - scala.math.pow((sigmatot+kivar)/(2*m),2)) - (sigmain/(2*m) - scala.math.pow(sigmatot/(2*m),2) - scala.math.pow(kivar/(2*m),2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST\n",
    "\n",
    "On veut savoir s'il est intéressant de placer le noeud 1 dans la communauté 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmain1: Double = 0.0\n",
       "sigmatot1: Double = 5.0\n",
       "kivar: Double = 16.0\n",
       "kiinvar: Double = 1.0\n",
       "m: Long = 39\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sigmain1 = sigmaIn1(1,14,newgraph)\n",
    "var sigmatot1 = sigmaTot1(1,14,newgraph)\n",
    "var kivar = ki(1, newgraph)\n",
    "var kiinvar = kiin2(1,14,newgraph)\n",
    "var m=newgraph.edges.count()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dq1: Double = -0.013477975016436564\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dq1 = DQ(sigmain1, sigmatot1, kivar, kiinvar, m , newgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut savoir quel noeud il faut bouger en priorité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: Int = 0\n",
       "setDq: Set[(Int, Double)] = Set((4,-0.018737672583826435), (16,0.002301117685733052), (11,-0.002958579881656806), (8,-0.008218277449046671), (34,-0.07659434582511504), (30,-0.008218277449046671), (22,0.002301117685733052), (17,0.002301117685733052), (5,-0.002958579881656806), (26,-0.002958579881656806), (20,-0.002958579881656806), (23,0.002301117685733052), (33,-0.05029585798816569), (6,-0.008218277449046671), (27,0.002301117685733052), (2,-0.03451676528599608), (25,-0.002958579881656806), (18,0.002301117685733052), (3,-0.03977646285338594), (9,-0.013477975016436564), (19,0.002301117685733052), (28,-0.008218277449046671), (21,0.002301117685733052), (7,-0.008218277449046671), (14,-0.013477975016436564), (31,-0.008218277449046671), (29,-0.002958579881656806), (15,0.002301117685...\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var a = 0\n",
    "var setDq =Set(): Set[(Int,Double)]\n",
    "for (a <- 2 to 34){\n",
    "    var sigmaindist = sigmaIn2(a.toLong, newgraph)\n",
    "    var sigmatotdist = sigmaTot2(a.toLong, newgraph)\n",
    "    var dq2 = DQ(sigmaindist, sigmatotdist, kivar, kiinvar, m, newgraph)\n",
    "    setDq+=((a,dq2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxCom: (setDq: Set[(Int, Double)])Int\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxCom(setDq : Set[(Int, Double)]) : Int = {\n",
    "    var max = -1000.0\n",
    "    var ind = -1\n",
    "    setDq.foreach(x => {\n",
    "        if (x._2>max)\n",
    "        {\n",
    "            ind=x._1\n",
    "            max=x._2\n",
    "        }\n",
    "    })\n",
    "    return ind\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Int = 12\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxCom(setDq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moveItoC: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])org.apache.spark.graphx.Graph[(Long, Int),Int]\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moveItoC(i:Int, comId : Int, graph:Graph[(Long, Int),Int]) : Graph[(Long, Int),Int] = {\n",
    "    val newVertices = graph.mapVertices { case (id, attr) => if (id==i) (comId.toLong, attr._2) else attr }\n",
    "    return newVertices\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On bouge le noeud 12 dans la communauté 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updated_graph: org.apache.spark.graphx.Graph[(Long, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@20960476\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var updated_graph = moveItoC(12,1,newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| _1|      _2|\n",
      "+---+--------+\n",
      "| 20| [20, 3]|\n",
      "| 13| [13, 2]|\n",
      "| 19| [19, 2]|\n",
      "| 34|[34, 17]|\n",
      "| 15| [15, 2]|\n",
      "|  4|  [4, 6]|\n",
      "| 21| [21, 2]|\n",
      "| 16| [16, 2]|\n",
      "| 22| [22, 2]|\n",
      "| 25| [25, 3]|\n",
      "| 28| [28, 4]|\n",
      "| 29| [29, 3]|\n",
      "| 11| [11, 3]|\n",
      "| 14| [14, 5]|\n",
      "| 32| [32, 6]|\n",
      "| 30| [30, 4]|\n",
      "| 24| [24, 5]|\n",
      "| 27| [27, 2]|\n",
      "| 33|[33, 12]|\n",
      "| 23| [23, 2]|\n",
      "|  1| [1, 16]|\n",
      "|  6|  [6, 4]|\n",
      "| 17| [17, 2]|\n",
      "|  3| [3, 10]|\n",
      "|  7|  [7, 4]|\n",
      "|  9|  [9, 5]|\n",
      "|  8|  [8, 4]|\n",
      "| 12|  [1, 1]|\n",
      "| 18| [18, 2]|\n",
      "| 31| [31, 4]|\n",
      "| 26| [26, 3]|\n",
      "| 10| [10, 2]|\n",
      "|  5|  [5, 3]|\n",
      "|  2|  [2, 9]|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_graph.vertices.toDF().show(74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
