{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://mbpdejebaptiste:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1618495077210)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions.split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karate_text: org.apache.spark.rdd.RDD[String] = data/soc-karate/soc-karate.mtx MapPartitionsRDD[1] at textFile at <console>:38\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val karate_text = sc.textFile(\"data/soc-karate/soc-karate.mtx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_splitted: org.apache.spark.sql.DataFrame = [srcId: string, dstID: string]\n",
       "rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[11] at rdd at <console>:51\n",
       "karate_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[12] at map at <console>:53\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_karate_text = karate_text.toDF.withColumn(\"id\",monotonicallyIncreasingId)\n",
    "\n",
    "\n",
    "df_karate_text = df_karate_text.withColumn(\"rank\", row_number().over(Window.orderBy(\"id\")))\n",
    "df_karate_text = df_karate_text.filter(df_karate_text(\"rank\")>24)\n",
    "df_karate_text = df_karate_text.drop(\"id\",\"rank\")\n",
    "\n",
    "val df_karate_splitted = df_karate_text.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")\n",
    "\n",
    "val rows: RDD[Row] = df_karate_splitted.rdd\n",
    "\n",
    "var karate_edges = rows.map{ case Row(src:String, dist : String) => Edge(src.toLong, dist.toLong,1)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default_user: (Int, Int) = (0,0)\n",
       "graph: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@4a4e91eb\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var default_user=(0, 0)\n",
    "var graph = Graph.fromEdges(karate_edges, default_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,1), Edge(3,1,1), Edge(3,2,1), Edge(4,1,1), Edge(4,2,1), Edge(4,3,1), Edge(5,1,1), Edge(6,1,1), Edge(7,1,1), Edge(7,5,1), Edge(7,6,1), Edge(8,1,1), Edge(8,2,1), Edge(8,3,1), Edge(8,4,1), Edge(9,1,1), Edge(9,3,1), Edge(10,3,1), Edge(11,1,1), Edge(11,5,1), Edge(11,6,1), Edge(12,1,1), Edge(13,1,1), Edge(13,4,1), Edge(14,1,1), Edge(14,2,1), Edge(14,3,1), Edge(14,4,1), Edge(17,6,1), Edge(17,7,1), Edge(18,1,1), Edge(18,2,1), Edge(20,1,1), Edge(20,2,1), Edge(22,1,1), Edge(22,2,1), Edge(26,24,1), Edge(26,25,1), Edge(28,3,1), Edge(28,24,1), Edge(28,25,1), Edge(29,3,1), Edge(30,24,1), Edge(30,27,1), Edge(31,2,1), Edge(31,9,1), Edge(32,1,1), Edge(32,25,1), Edge(32,26,1), Edge(32,29,1), Edge(33,3,1), Edge(33,9,1), Edge(33,15,1), Edge(33...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(org.apache.spark.graphx.VertexId, (Int, Int))] = Array((20,(0,0)), (13,(0,0)), (19,(0,0)), (34,(0,0)), (15,(0,0)), (4,(0,0)), (21,(0,0)), (16,(0,0)), (22,(0,0)), (25,(0,0)), (28,(0,0)), (29,(0,0)), (11,(0,0)), (14,(0,0)), (32,(0,0)), (30,(0,0)), (24,(0,0)), (27,(0,0)), (33,(0,0)), (23,(0,0)), (1,(0,0)), (6,(0,0)), (17,(0,0)), (3,(0,0)), (7,(0,0)), (9,(0,0)), (8,(0,0)), (12,(0,0)), (18,(0,0)), (31,(0,0)), (26,(0,0)), (10,(0,0)), (5,(0,0)), (2,(0,0)))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des degrés des noeuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newgraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@145473c9\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newgraph = graph.outerJoinVertices(graph.degrees)((index,_,deg) => (index,deg.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[org.apache.spark.graphx.EdgeTriplet[(org.apache.spark.graphx.VertexId, Int),Int]] = Array(((2,(2,9)),(1,(1,16)),1), ((3,(3,10)),(1,(1,16)),1), ((3,(3,10)),(2,(2,9)),1), ((4,(4,6)),(1,(1,16)),1), ((4,(4,6)),(2,(2,9)),1), ((4,(4,6)),(3,(3,10)),1), ((5,(5,3)),(1,(1,16)),1), ((6,(6,4)),(1,(1,16)),1), ((7,(7,4)),(1,(1,16)),1), ((7,(7,4)),(5,(5,3)),1), ((7,(7,4)),(6,(6,4)),1), ((8,(8,4)),(1,(1,16)),1), ((8,(8,4)),(2,(2,9)),1), ((8,(8,4)),(3,(3,10)),1), ((8,(8,4)),(4,(4,6)),1), ((9,(9,5)),(1,(1,16)),1), ((9,(9,5)),(3,(3,10)),1), ((10,(10,2)),(3,(3,10)),1), ((11,(11,3)),(1,(1,16)),1), ((11,(11,3)),(5,(5,3)),1), ((11,(11,3)),(6,(6,4)),1), ((12,(12,1)),(1,(1,16)),1), ((13,(13,2)),(1,(1,16)),1), ((13,(13,2)),(4,(4,6)),1), ((14,(14,5)),(1,(1,16)),1), ((14,(14,5)),(2,(2,9)),1), ((14,(14,...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgraph.triplets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(org.apache.spark.graphx.VertexId, (org.apache.spark.graphx.VertexId, Int))] = Array((20,(20,3)), (13,(13,2)), (19,(19,2)), (34,(34,17)), (15,(15,2)), (4,(4,6)), (21,(21,2)), (16,(16,2)), (22,(22,2)), (25,(25,3)), (28,(28,4)), (29,(29,3)), (11,(11,3)), (14,(14,5)), (32,(32,6)), (30,(30,4)), (24,(24,5)), (27,(27,2)), (33,(33,12)), (23,(23,2)), (1,(1,16)), (6,(6,4)), (17,(17,2)), (3,(3,10)), (7,(7,4)), (9,(9,5)), (8,(8,4)), (12,(12,1)), (18,(18,2)), (31,(31,4)), (26,(26,3)), (10,(10,2)), (5,(5,3)), (2,(2,9)))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgraph.vertices.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 1: Variables utiles pour calculer le gain en modularité lors de la suppression d'un noeud d'une communauté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaIn1: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "sigmaTot1: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmaIn1(i:Long, comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1==comId) &&\n",
    "                                                    !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def sigmaTot1(i:Long, comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId) && !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 2: Variables utiles pour calculer le gain en modularité lors de l'ajout d'un noeud à une communauté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaIn2: (comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "kiin2: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "sigmaTot2: (comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmaIn2(comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcAttr._1==comId && triplet.dstAttr._1==comId))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin2(i:Long, comId : Long, graph : Graph[(Long, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstAttr._1==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def sigmaTot2(comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables utiles au 2 étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ki: (i: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "kiin: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ki(i: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcId==i || triplet.dstId==i))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin(i:Long, comId : Long, graph : Graph[(Long, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstAttr._1==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du différentiel de modularité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dq: (sigmain: Double, sigmatot: Double, kivar: Double, kiinvar: Double, m: Long)Double\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dq(sigmain:Double, sigmatot:Double, kivar:Double, kiinvar:Double, m:Long):Double = {\n",
    "    return (((sigmain+kiinvar)/(2*m) - scala.math.pow((sigmatot+kivar)/(2*m),2)) - (sigmain/(2*m) - scala.math.pow(sigmatot/(2*m),2) - scala.math.pow(kivar/(2*m),2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxCom: (setDq: Set[(Int, Double)])(Int, Double)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxCom(setDq : Set[(Int, Double)]) : (Int, Double) = {\n",
    "    var max = -1000.0\n",
    "    var ind = -1\n",
    "    setDq.foreach(x => {\n",
    "        if (x._2>max)\n",
    "        {\n",
    "            ind=x._1\n",
    "            max=x._2\n",
    "        }\n",
    "    })\n",
    "    return (ind,max)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moveItoC: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, Int),Int])org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, Int),Int]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moveItoC(i:Int, comId : Int, graph:Graph[(VertexId, Int),Int]) : Graph[(VertexId, Int),Int] = {\n",
    "    val newVertices = graph.mapVertices { case (id, attr) => if (id==i) (comId.toLong, attr._2) else attr }\n",
    "    return newVertices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "communities: (graph: org.apache.spark.graphx.Graph[(Long, Int),Int])org.apache.spark.graphx.Graph[(Long, Int),Int]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def communities(graph:Graph[(Long,Int),Int]) : Graph[(Long,Int),Int] = {\n",
    "    var currentGraph = graph\n",
    "    var nbVertices = graph.vertices.count\n",
    "    var m = graph.edges.count\n",
    "\n",
    "    var i = 0\n",
    "    var keepGoing = true\n",
    "    var j=0\n",
    "    for (j <- 1 to 2){\n",
    "        println(j)\n",
    "        keepGoing=false\n",
    "        println(((currentGraph.vertices.collect { case t => t._2._1 }).collect.distinct).size)\n",
    "        for (i<- 1 to nbVertices.toInt){\n",
    "            println(\"Noeud considéré : \"+i+ '\\n')\n",
    "            /*get the community and ki */\n",
    "            var vertice = (currentGraph.vertices.collect{ case t if t._1 == i => t._2}).collect\n",
    "            var comId = vertice(0)._1.toInt\n",
    "            println(\"Communauté de \"+i+\" : \"+comId)\n",
    "            var kivar = vertice(0)._2\n",
    "\n",
    "            /* calculate dq1 */\n",
    "            var kiin1 = kiin(i, comId, currentGraph)\n",
    "            var simgain1 = sigmaIn1(i, comId, currentGraph)\n",
    "            var simgatot1 = sigmaTot1(i, comId, currentGraph)\n",
    "            var dq1 = dq(sigmain1, sigmatot1, kivar, kiin1, m)\n",
    "\n",
    "            /*look all combis : we need to know how many communities are left and which one*/\n",
    "            var communities = (currentGraph.vertices.collect { case t => t._2._1 }).collect.distinct\n",
    "\n",
    "            var setDq =Set(): Set[(Int,Double)]\n",
    "            communities.foreach( a => {\n",
    "                if (a!=comId){\n",
    "                    var sigmain2 = sigmaIn2(a, newgraph)\n",
    "                    var sigmatot2 = sigmaTot2(a, newgraph)\n",
    "                    var kiin2 = kiin(i,a, newgraph)\n",
    "                    var dq2 = dq(sigmain2, sigmatot2, kivar, kiin2, m)\n",
    "                    setDq+=((a.toInt,dq2))\n",
    "                }\n",
    "            })\n",
    "\n",
    "            /* get best combi and move if larger than 0 */\n",
    "            var bestCombi = maxCom(setDq)\n",
    "            if (bestCombi._2 > 0 ) {\n",
    "                \n",
    "                var communities = (currentGraph.vertices.collect { case t => t._2._1 }).collect.distinct\n",
    "                println(\"On met \"+ i +\" dans \"+bestCombi._1+ '\\n')\n",
    "                var updatedGraph = moveItoC(i, bestCombi._1, currentGraph)\n",
    "                currentGraph=updatedGraph\n",
    "\n",
    "                /*Un changement est fait*/\n",
    "                keepGoing=true\n",
    "\n",
    "            }\n",
    "            \n",
    "    }\n",
    "    println(((currentGraph.vertices.collect { case t => t._2._1 }).collect.distinct).size)\n",
    "    }\n",
    "    return currentGraph\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST\n",
    "\n",
    "On veut savoir s'il est intéressant de placer le noeud 1 dans la communauté 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmain1: Double = 0.0\n",
       "sigmatot1: Double = 5.0\n",
       "kivar: Double = 16.0\n",
       "kiinvar: Double = 1.0\n",
       "m: Long = 39\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sigmain1 = sigmaIn1(1,14,newgraph)\n",
    "var sigmatot1 = sigmaTot1(1,14,newgraph)\n",
    "var kivar = ki(1, newgraph)\n",
    "var kiinvar = kiin2(1,14,newgraph)\n",
    "var m=newgraph.edges.count()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dq1: Double = -0.013477975016436564\n"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dq1 = DQ(sigmain1, sigmatot1, kivar, kiinvar, m , newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "34\n",
      "Noeud considéré : 1\n",
      "\n",
      "Communauté de 1 : 1\n"
     ]
    }
   ],
   "source": [
    "var a =communities(newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a.vertices.collect{ case t => t._2._1 }).collect.distinct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
