{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://mbpdejebaptiste:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1618523076483)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.functions.split\n",
       "import org.apache.spark.graphx.EdgeDirection\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions.split\n",
    "import org.apache.spark.graphx.EdgeDirection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données Karate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karate_text: org.apache.spark.rdd.RDD[String] = data/soc-karate/soc-karate.mtx MapPartitionsRDD[1] at textFile at <console>:39\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val karate_text = sc.textFile(\"data/soc-karate/soc-karate.mtx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_number: String = 0\n",
       "facebook_text: org.apache.spark.rdd.RDD[String] = data/facebook_data/facebook/0.edges MapPartitionsRDD[3] at textFile at <console>:41\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//choix du fichier\n",
    "var file_number = \"0\"\n",
    "var facebook_text = sc.textFile(\"data/facebook_data/facebook/\" + file_number + \".edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_karate_splitted: org.apache.spark.sql.DataFrame = [srcId: string, dstID: string]\n",
       "rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[13] at rdd at <console>:52\n",
       "karate_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[14] at map at <console>:54\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_karate_text = karate_text.toDF.withColumn(\"id\",monotonicallyIncreasingId)\n",
    "\n",
    "\n",
    "df_karate_text = df_karate_text.withColumn(\"rank\", row_number().over(Window.orderBy(\"id\")))\n",
    "df_karate_text = df_karate_text.filter(df_karate_text(\"rank\")>24)\n",
    "df_karate_text = df_karate_text.drop(\"id\",\"rank\")\n",
    "\n",
    "val df_karate_splitted = df_karate_text.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")\n",
    "\n",
    "val rows: RDD[Row] = df_karate_splitted.rdd\n",
    "\n",
    "var karate_edges = rows.map{ case Row(src:String, dist : String) => Edge(src.toLong, dist.toLong,1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facebook_text: org.apache.spark.rdd.RDD[String] = data/facebook_data/facebook/0.edges MapPartitionsRDD[16] at textFile at <console>:42\n",
       "df_facebook_text: org.apache.spark.sql.DataFrame = [value: string]\n",
       "df_facebook_text_splitted: org.apache.spark.sql.DataFrame = [srcId: string, dstID: string]\n",
       "facebook_rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[21] at rdd at <console>:50\n",
       "facebook_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[22] at map at <console>:52\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var facebook_text = sc.textFile(\"data/facebook_data/facebook/\" + file_number + \".edges\")\n",
    "var df_facebook_text = facebook_text.toDF()\n",
    "\n",
    "var df_facebook_text_splitted = df_facebook_text.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")\n",
    "\n",
    "val facebook_rows: RDD[Row] = df_facebook_text_splitted.rdd\n",
    "\n",
    "var facebook_edges = facebook_rows.map{ case Row(src:String, dist : String) => Edge(src.toLong, dist.toLong,1)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chosen_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[14] at map at <console>:54\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var chosen_edges = karate_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default_user: (Int, Int) = (0,0)\n",
       "graph: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@512ed8c3\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var default_user=(0, 0)\n",
    "var graph = Graph.fromEdges(chosen_edges, default_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,1), Edge(3,1,1), Edge(3,2,1), Edge(4,1,1), Edge(4,2,1), Edge(4,3,1), Edge(5,1,1), Edge(6,1,1), Edge(7,1,1), Edge(7,5,1), Edge(7,6,1), Edge(8,1,1), Edge(8,2,1), Edge(8,3,1), Edge(8,4,1), Edge(9,1,1), Edge(9,3,1), Edge(10,3,1), Edge(11,1,1), Edge(11,5,1), Edge(11,6,1), Edge(12,1,1), Edge(13,1,1), Edge(13,4,1), Edge(14,1,1), Edge(14,2,1), Edge(14,3,1), Edge(14,4,1), Edge(17,6,1), Edge(17,7,1), Edge(18,1,1), Edge(18,2,1), Edge(20,1,1), Edge(20,2,1), Edge(22,1,1), Edge(22,2,1), Edge(26,24,1), Edge(26,25,1), Edge(28,3,1), Edge(28,24,1), Edge(28,25,1), Edge(29,3,1), Edge(30,24,1), Edge(30,27,1), Edge(31,2,1), Edge(31,9,1), Edge(32,1,1), Edge(32,25,1), Edge(32,26,1), Edge(32,29,1), Edge(33,3,1), Edge(33,9,1), Edge(33,15,1), Edge(33...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(org.apache.spark.graphx.VertexId, (Int, Int))] = Array((20,(0,0)), (13,(0,0)), (19,(0,0)), (34,(0,0)), (15,(0,0)), (4,(0,0)), (21,(0,0)), (16,(0,0)), (22,(0,0)), (25,(0,0)), (28,(0,0)), (29,(0,0)), (11,(0,0)), (14,(0,0)), (32,(0,0)), (30,(0,0)), (24,(0,0)), (27,(0,0)), (33,(0,0)), (23,(0,0)), (1,(0,0)), (6,(0,0)), (17,(0,0)), (3,(0,0)), (7,(0,0)), (9,(0,0)), (8,(0,0)), (12,(0,0)), (18,(0,0)), (31,(0,0)), (26,(0,0)), (10,(0,0)), (5,(0,0)), (2,(0,0)))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des degrés des noeuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newgraph: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@157f2644\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newgraph = graph.outerJoinVertices(graph.degrees)((index,_,deg) => (index.toInt,deg.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[org.apache.spark.graphx.EdgeTriplet[(Int, Int),Int]] = Array(((2,(2,9)),(1,(1,16)),1), ((3,(3,10)),(1,(1,16)),1), ((3,(3,10)),(2,(2,9)),1), ((4,(4,6)),(1,(1,16)),1), ((4,(4,6)),(2,(2,9)),1), ((4,(4,6)),(3,(3,10)),1), ((5,(5,3)),(1,(1,16)),1), ((6,(6,4)),(1,(1,16)),1), ((7,(7,4)),(1,(1,16)),1), ((7,(7,4)),(5,(5,3)),1), ((7,(7,4)),(6,(6,4)),1), ((8,(8,4)),(1,(1,16)),1), ((8,(8,4)),(2,(2,9)),1), ((8,(8,4)),(3,(3,10)),1), ((8,(8,4)),(4,(4,6)),1), ((9,(9,5)),(1,(1,16)),1), ((9,(9,5)),(3,(3,10)),1), ((10,(10,2)),(3,(3,10)),1), ((11,(11,3)),(1,(1,16)),1), ((11,(11,3)),(5,(5,3)),1), ((11,(11,3)),(6,(6,4)),1), ((12,(12,1)),(1,(1,16)),1), ((13,(13,2)),(1,(1,16)),1), ((13,(13,2)),(4,(4,6)),1), ((14,(14,5)),(1,(1,16)),1), ((14,(14,5)),(2,(2,9)),1), ((14,(14,5)),(3,(3,10)),1), ((14,(14,5...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgraph.triplets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(org.apache.spark.graphx.VertexId, (Int, Int))] = Array((20,(20,3)), (13,(13,2)), (19,(19,2)), (34,(34,17)), (15,(15,2)), (4,(4,6)), (21,(21,2)), (16,(16,2)), (22,(22,2)), (25,(25,3)), (28,(28,4)), (29,(29,3)), (11,(11,3)), (14,(14,5)), (32,(32,6)), (30,(30,4)), (24,(24,5)), (27,(27,2)), (33,(33,12)), (23,(23,2)), (1,(1,16)), (6,(6,4)), (17,(17,2)), (3,(3,10)), (7,(7,4)), (9,(9,5)), (8,(8,4)), (12,(12,1)), (18,(18,2)), (31,(31,4)), (26,(26,3)), (10,(10,2)), (5,(5,3)), (2,(2,9)))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgraph.vertices.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 1: Variables utiles pour calculer le gain en modularité lors de la suppression d'un noeud d'une communauté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaIn1: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n",
       "sigmaTot1: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmaIn1(i:Int, comId: Int, graph : Graph[(Int, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1==comId) &&\n",
    "                                                    !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def sigmaTot1(i:Int, comId: Int, graph : Graph[(Int, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => \n",
    "    (triplet.srcAttr._1==comId && triplet.srcId !=i && triplet.dstAttr._1 !=comId) || \n",
    "    (triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId && triplet.dstId !=i) ||\n",
    "    (triplet.srcId==i && triplet.dstAttr._1==comId) || \n",
    "    (triplet.srcAttr._1==comId && triplet.dstId==i ) \n",
    "    ))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 2: Variables utiles pour calculer le gain en modularité lors de l'ajout d'un noeud à une communauté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmaIn2: (comId: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n",
       "sigmaTot2: (comId: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmaIn2(comId: Int, graph : Graph[(Int, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcAttr._1==comId && triplet.dstAttr._1==comId))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "\n",
    "def sigmaTot2(comId: Int, graph : Graph[(Int, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables utiles au 2 étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ki: (i: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n",
       "kiin: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ki(i: Int, graph : Graph[(Int, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcId==i || triplet.dstId==i))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin(i:Int, comId : Int, graph : Graph[(Int, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcId==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du différentiel de modularité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dq: (sigmain: Double, sigmatot: Double, kivar: Double, kiinvar: Double, m: Double)Double\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dq(sigmain:Double, sigmatot:Double, kivar:Double, kiinvar:Double, m:Double):Double = {\n",
    "    return (((sigmain+kiinvar)/(2*m) - scala.math.pow((sigmatot+kivar)/(2*m),2)) - (sigmain/(2*m) - scala.math.pow(sigmatot/(2*m),2) - scala.math.pow(kivar/(2*m),2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxCom: (setDq: Set[(Int, Double)])(Int, Double)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxCom(setDq : Set[(Int, Double)]) : (Int, Double) = {\n",
    "    var max = -1000.0\n",
    "    var ind = -1\n",
    "    setDq.foreach(x => {\n",
    "        if (x._2>max)\n",
    "        {\n",
    "            ind=x._1\n",
    "            max=x._2\n",
    "        }\n",
    "    })\n",
    "    return (ind,max)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moveItoC: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(Int, Int),Int])org.apache.spark.graphx.Graph[(Int, Int),Int]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moveItoC(i:Int, comId : Int, graph:Graph[(Int, Int),Int]) : Graph[(Int, Int),Int] = {\n",
    "    val newVertices = graph.mapVertices { case (id, attr) => if (id==i) (comId, attr._2) else attr }\n",
    "    return newVertices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q: (graph: org.apache.spark.graphx.Graph[(Int, Int),Int])Double\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q(graph:Graph[(Int,Int),Int]) : Double = {\n",
    "    var m = graph.edges.count()\n",
    "    var vertices = graph.vertices.collect\n",
    "    var q=0.0\n",
    "    vertices.foreach{ case t1 => {\n",
    "        vertices.foreach{ case t2 => {\n",
    "\n",
    "            var isEdge = graph.edges.filter(e => (e.srcId == t1._1 &&e.dstId==t2._1) || (e.dstId==t2._1 && e.srcId==t1._1)).count.toDouble\n",
    "            if (t1._2._1 == t2._2._1){\n",
    "                q+= isEdge - t1._2._2.toDouble*t2._2._2.toDouble/(2*m)\n",
    "            }\n",
    "            \n",
    "\n",
    "            }\n",
    "        }   \n",
    "    \n",
    "        }\n",
    "    }\n",
    "    return q/(2*m)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "40: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:40: error: type mismatch;",
      " found   : (org.apache.spark.graphx.VertexId, (Int, Int))",
      "    (which expands to)  (Long, (Int, Int))",
      " required: (Int, Int)",
      "                   if (a._1 == 1) a else b",
      "                                  ^",
      "<console>:40: error: type mismatch;",
      " found   : (org.apache.spark.graphx.VertexId, (Int, Int))",
      "    (which expands to)  (Long, (Int, Int))",
      " required: (Int, Int)",
      "                   if (a._1 == 1) a else b",
      "                                         ^",
      "<console>:45: error: type mismatch;",
      " found   : (Int, Int)",
      " required: (org.apache.spark.graphx.VertexId, (Int, Int))",
      "    (which expands to)  (Long, (Int, Int))",
      "       newgraph.vertices.reduce(getVertexI)",
      "                                ^",
      ""
     ]
    }
   ],
   "source": [
    "def getVertexI(a : (VertexId, (Int, Int)), b : (VertexId, (Int, Int))): (Int, Int) = {\n",
    "            if (a._1 == 1) a else b\n",
    "            }\n",
    "\n",
    "newgraph.vertices.reduce(getVertexI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "communities: (graph: org.apache.spark.graphx.Graph[(Int, Int),Int])(org.apache.spark.graphx.Graph[(Int, Int),Int], Boolean)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def communities(graph:Graph[(Int,Int),Int]) : (Graph[(Int,Int),Int],Boolean) = {\n",
    "    var currentGraph = graph\n",
    "    val nbVertices = graph.vertices.count\n",
    "    val m = graph.edges.count\n",
    "\n",
    "    var i = 0\n",
    "    var keepGoing = true\n",
    "    //var j=0\n",
    "    \n",
    "    //println(j)\n",
    "    keepGoing=false\n",
    "    println(\"nombre actuel de communautées: \"+((currentGraph.vertices.collect { case t => t._2._1 }).collect.distinct).size)\n",
    "    for (i<- 1 to nbVertices.toInt){\n",
    "        println(\"Noeud considéré : \"+i+ '\\n')\n",
    "        /*get the community and ki */\n",
    "        /*def getVertexI(a: (Int, Int), b: (Int, Int)): (Int, Int) = {\n",
    "            if (a._1 == i) a else b\n",
    "            }*/\n",
    "        val vertice = currentGraph.vertices.filter{case (id, t) => id == i}.map{case (id, t) => t}.collect\n",
    "        val comId = vertice(0)._1\n",
    "        println(\"Communauté de \"+i+\" : \"+comId)\n",
    "        val kivar = vertice(0)._2\n",
    "\n",
    "        /* calculate dq1 */\n",
    "        val kiin1 = kiin(i, comId, currentGraph)\n",
    "        val sigmain1 = sigmaIn1(i, comId, currentGraph)\n",
    "        val sigmatot1 = sigmaTot1(i, comId, currentGraph)\n",
    "        val dq1 = dq(sigmain1, sigmatot1, kivar, kiin1, m)\n",
    "\n",
    "        /*look all combis : we need to know how many communities are left and which one*/\n",
    "        //val communities_available = (currentGraph.vertices.collect { case t => t._2._1 }).collect.distinct\n",
    "        val direction = EdgeDirection.Either \n",
    "        val communities_available = currentGraph.collectNeighbors(direction).lookup(i)(0).map{case (id, t) => t._1}\n",
    "        var setDq =Set(): Set[(Int,Double)]\n",
    "        communities_available.foreach( a => {\n",
    "            if (a!=comId){\n",
    "                val sigmain2 = sigmaIn2(a, currentGraph)\n",
    "                val sigmatot2 = sigmaTot2(a, currentGraph)\n",
    "                val kiin2 = kiin(i,a, currentGraph)\n",
    "                val dq2 = dq(sigmain2, sigmatot2, kivar, kiin2, m)\n",
    "                setDq+=((a.toInt,dq2-dq1))\n",
    "            }\n",
    "        })\n",
    "\n",
    "        /* get best combi and move if larger than 0 */\n",
    "        val bestCombi = maxCom(setDq)\n",
    "        if (bestCombi._2.toDouble > 0.0001 ) {\n",
    "            \n",
    "            println(\"On met \"+ i +\" dans \"+bestCombi._1+ '\\n')\n",
    "            var updatedGraph = moveItoC(i, bestCombi._1, currentGraph)\n",
    "            currentGraph=updatedGraph\n",
    "\n",
    "            /*Un changement est fait*/\n",
    "            keepGoing=true\n",
    "\n",
    "        }\n",
    "        println(\"Q = \"+q(currentGraph))\n",
    "    }\n",
    "    println(\"Q = \"+q(currentGraph))\n",
    "    return (currentGraph, keepGoing)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 1\n",
      "nombre actuel de communautées: 34\n",
      "Noeud considéré : 1\n",
      "\n",
      "Communauté de 1 : 1\n",
      "On met 1 dans 12\n",
      "\n",
      "Q = -0.04470742932281392\n",
      "Noeud considéré : 2\n",
      "\n",
      "Communauté de 2 : 2\n",
      "On met 2 dans 22\n",
      "\n",
      "Q = -0.03977646285338591\n",
      "Noeud considéré : 3\n",
      "\n",
      "Communauté de 3 : 3\n",
      "On met 3 dans 10\n",
      "\n",
      "Q = -0.035009861932938834\n",
      "Noeud considéré : 4\n",
      "\n",
      "Communauté de 4 : 4\n",
      "On met 4 dans 13\n",
      "\n",
      "Q = -0.02958579881656804\n",
      "Noeud considéré : 5\n",
      "\n",
      "Communauté de 5 : 5\n",
      "On met 5 dans 11\n",
      "\n",
      "Q = -0.02391518737672583\n",
      "Noeud considéré : 6\n",
      "\n",
      "Communauté de 6 : 6\n",
      "On met 6 dans 17\n",
      "\n",
      "Q = -0.018162393162393157\n",
      "Noeud considéré : 7\n",
      "\n",
      "Communauté de 7 : 7\n",
      "On met 7 dans 17\n",
      "\n",
      "Q = -0.007314266929651547\n",
      "Noeud considéré : 8\n",
      "\n",
      "Communauté de 8 : 8\n",
      "On met 8 dans 13\n",
      "\n",
      "Q = -0.0035338593030900724\n",
      "Noeud considéré : 9\n",
      "\n",
      "Communauté de 9 : 9\n",
      "On met 9 dans 31\n",
      "\n",
      "Q = 0.0012327416173570011\n",
      "Noeud considéré : 10\n",
      "\n",
      "Communauté de 10 : 10\n",
      "Q = 0.0012327416173570011\n",
      "Noeud considéré : 11\n",
      "\n",
      "Communauté de 11 : 11\n",
      "Q = 0.0012327416173570011\n",
      "Noeud considéré : 12\n",
      "\n",
      "Communauté de 12 : 12\n",
      "Q = 0.0012327416173570011\n",
      "Noeud considéré : 13\n",
      "\n",
      "Communauté de 13 : 13\n"
     ]
    }
   ],
   "source": [
    "var keep_going = true\n",
    "var updated_graph = newgraph\n",
    "var iteration = 0\n",
    "\n",
    "while (keep_going){\n",
    "    iteration += 1 \n",
    "    println(\"ITERATION: \"+iteration)\n",
    "    var community_tuple = communities(updated_graph)\n",
    "    updated_graph = community_tuple._1\n",
    "    keep_going = community_tuple._2\n",
    "    if (iteration>0){\n",
    "        keep_going = false}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST\n",
    "\n",
    "On veut savoir s'il est intéressant de placer le noeud 1 dans la communauté 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmain2: Double = 0.0\n",
       "sigmatot2: Double = 1.0\n",
       "kivar: Double = 16.0\n",
       "kiinvar: Double = 1.0\n",
       "m: Long = 78\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sigmain2 = sigmaIn2(12,newgraph)\n",
    "var sigmatot2 = sigmaTot2(12,newgraph)\n",
    "var kivar = ki(1, newgraph)\n",
    "var kiinvar = kiin(1,12,newgraph)\n",
    "var m=newgraph.edges.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var dq1 = dq(sigmain2, sigmatot2, kivar, kiinvar, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dq2: Double = 0.00509533201840894\n"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dq2 = dq(sigmain2, sigmatot2, kivar, kiinvar, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: Double = -0.04980276134122287\n"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var a = q(newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graphCible: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@648efdb2\n"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var graphCible = moveItoC(1,12,newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b: Double = -0.04470742932281392\n"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var b = q(graphCible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Double = -0.04470742932281393\n"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+dq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0013149243918474714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sigmain2: Double = 0.0\n",
       "sigmatot2: Double = 2.0\n",
       "kivar: Double = 16.0\n",
       "kiinvar2: Double = 1.0\n",
       "m: Double = 78.0\n",
       "dq2: Double = 0.003780407626561469\n",
       "sigmain1: Double = 0.0\n",
       "sigmatot1: Double = 1.0\n",
       "kiinvar1: Double = 1.0\n",
       "dq1: Double = 0.00509533201840894\n",
       "dqtot: Double = -0.0013149243918474714\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sigmain2 = sigmaIn2(13,graphCible)\n",
    "var sigmatot2 = sigmaTot2(13,graphCible)\n",
    "var kivar = ki(1, graphCible)\n",
    "var kiinvar2 = kiin(1,13,graphCible)\n",
    "var m=graphCible.edges.count.toDouble\n",
    "var dq2 = dq(sigmain2, sigmatot2, kivar, kiinvar2, m)\n",
    "\n",
    "var sigmain1 = sigmaIn1(1,12,graphCible)\n",
    "var sigmatot1 = sigmaTot1(1,12,graphCible)\n",
    "var kiinvar1 = kiin(1,12,graphCible)\n",
    "\n",
    "var dq1= dq(sigmain1, sigmatot1, kivar, kiinvar1, m)\n",
    "\n",
    "var dqtot = dq2 - dq1\n",
    "\n",
    "println(dqtot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "communities_available: Array[Int] = Array(2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 18, 20, 22, 32)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var communities_available = newgraph.collectNeighbors(direction).lookup(1)(0).map{case (id, t) => t._1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "direction: org.apache.spark.graphx.EdgeDirection = EdgeDirection.Either\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        val direction = EdgeDirection.Either \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|srcId|dstId|attr|\n",
      "+-----+-----+----+\n",
      "|    2|    1|   1|\n",
      "|    3|    1|   1|\n",
      "|    3|    2|   1|\n",
      "|    4|    1|   1|\n",
      "|    4|    2|   1|\n",
      "|    4|    3|   1|\n",
      "|    5|    1|   1|\n",
      "|    6|    1|   1|\n",
      "|    7|    1|   1|\n",
      "|    7|    5|   1|\n",
      "|    7|    6|   1|\n",
      "|    8|    1|   1|\n",
      "|    8|    2|   1|\n",
      "|    8|    3|   1|\n",
      "|    8|    4|   1|\n",
      "|    9|    1|   1|\n",
      "|    9|    3|   1|\n",
      "|   10|    3|   1|\n",
      "|   11|    1|   1|\n",
      "|   11|    5|   1|\n",
      "|   11|    6|   1|\n",
      "|   12|    1|   1|\n",
      "|   13|    1|   1|\n",
      "|   13|    4|   1|\n",
      "|   14|    1|   1|\n",
      "|   14|    2|   1|\n",
      "|   14|    3|   1|\n",
      "|   14|    4|   1|\n",
      "|   17|    6|   1|\n",
      "|   17|    7|   1|\n",
      "|   18|    1|   1|\n",
      "|   18|    2|   1|\n",
      "|   20|    1|   1|\n",
      "|   20|    2|   1|\n",
      "|   22|    1|   1|\n",
      "|   22|    2|   1|\n",
      "|   26|   24|   1|\n",
      "|   26|   25|   1|\n",
      "|   28|    3|   1|\n",
      "|   28|   24|   1|\n",
      "|   28|   25|   1|\n",
      "|   29|    3|   1|\n",
      "|   30|   24|   1|\n",
      "|   30|   27|   1|\n",
      "|   31|    2|   1|\n",
      "|   31|    9|   1|\n",
      "|   32|    1|   1|\n",
      "|   32|   25|   1|\n",
      "|   32|   26|   1|\n",
      "|   32|   29|   1|\n",
      "|   33|    3|   1|\n",
      "|   33|    9|   1|\n",
      "|   33|   15|   1|\n",
      "|   33|   16|   1|\n",
      "|   33|   19|   1|\n",
      "|   33|   21|   1|\n",
      "|   33|   23|   1|\n",
      "|   33|   24|   1|\n",
      "|   33|   30|   1|\n",
      "|   33|   31|   1|\n",
      "|   33|   32|   1|\n",
      "|   34|    9|   1|\n",
      "|   34|   10|   1|\n",
      "|   34|   14|   1|\n",
      "|   34|   15|   1|\n",
      "|   34|   16|   1|\n",
      "|   34|   19|   1|\n",
      "|   34|   20|   1|\n",
      "|   34|   21|   1|\n",
      "|   34|   23|   1|\n",
      "|   34|   24|   1|\n",
      "|   34|   27|   1|\n",
      "|   34|   28|   1|\n",
      "|   34|   29|   1|\n",
      "|   34|   30|   1|\n",
      "|   34|   31|   1|\n",
      "|   34|   32|   1|\n",
      "|   34|   33|   1|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newgraph.edges.toDF.show(78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ceci: Array[(org.apache.spark.graphx.VertexId, (Int, Int))] = Array((2,(2,9)), (3,(3,10)), (4,(4,6)), (5,(5,3)), (6,(6,4)), (7,(7,4)), (8,(8,4)), (9,(9,5)), (11,(11,3)), (12,(12,1)), (13,(13,2)), (14,(14,5)), (18,(18,2)), (20,(20,3)), (22,(22,2)), (32,(32,6)))\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var ceci = newgraph.collectNeighbors(direction).lookup(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
