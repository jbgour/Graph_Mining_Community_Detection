{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions.split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "karate_graph: org.apache.spark.rdd.RDD[String] = data/soc-karate/soc-karate.mtx MapPartitionsRDD[1] at textFile at <console>:38\n"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "val karate_graph = sc.textFile(\"data/soc-karate/soc-karate.mtx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "df_karate: org.apache.spark.sql.DataFrame = [value: string, id: bigint]\n"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "var df_karate = karate_graph.toDF.withColumn(\"id\",monotonicallyIncreasingId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "df_karate: org.apache.spark.sql.DataFrame = [value: string, id: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_karate = df_karate.withColumn(\"rank\", row_number().over(Window.orderBy(\"id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------------------------------------------------------+----------+----+\n|value                                                                           |id        |rank|\n+--------------------------------------------------------------------------------+----------+----+\n|%%MatrixMarket matrix coordinate pattern symmetric                              |0         |1   |\n|%-------------------------------------------------------------------------------|1         |2   |\n|% UF Sparse Matrix Collection, Tim Davis                                        |2         |3   |\n|% http://www.cise.ufl.edu/research/sparse/matrices/Newman/karate                |3         |4   |\n|% name: Newman/karate                                                           |4         |5   |\n|% [Karate club, from Wayne Zachary, 1977]                                       |5         |6   |\n|% id: 2399                                                                      |6         |7   |\n|% date: 1977                                                                    |7         |8   |\n|% author: W. Zachary                                                            |8         |9   |\n|% ed: M. Newman                                                                 |9         |10  |\n|% fields: name title A id date author kind notes ed                             |10        |11  |\n|% kind: undirected graph                                                        |11        |12  |\n|%-------------------------------------------------------------------------------|12        |13  |\n|% notes:                                                                        |13        |14  |\n|% Network collection from M. Newman                                             |14        |15  |\n|% http://www-personal.umich.edu/~mejn/netdata/                                  |15        |16  |\n|%                                                                               |16        |17  |\n|% The graph \"karate\" contains the network of friendships between the 34         |17        |18  |\n|% members of a karate club at a US university, as described by Wayne Zachary    |8589934592|19  |\n|% in 1977.  If you use these data in your work, please cite W. W. Zachary, An   |8589934593|20  |\n|% information flow model for conflict and fission in small groups, Journal of   |8589934594|21  |\n|% Anthropological Research 33, 452-473 (1977).                                  |8589934595|22  |\n|%-------------------------------------------------------------------------------|8589934596|23  |\n|34 34 78                                                                        |8589934597|24  |\n|2 1                                                                             |8589934598|25  |\n|3 1                                                                             |8589934599|26  |\n|4 1                                                                             |8589934600|27  |\n|5 1                                                                             |8589934601|28  |\n|6 1                                                                             |8589934602|29  |\n|7 1                                                                             |8589934603|30  |\n+--------------------------------------------------------------------------------+----------+----+\nonly showing top 30 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_karate.show(30,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "df_karate: org.apache.spark.sql.DataFrame = [value: string, id: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_karate = df_karate.filter(df_karate(\"rank\")>24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "df_karate: org.apache.spark.sql.DataFrame = [value: string]\n"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df_karate = df_karate.drop(\"id\",\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "df_karate_splitted: org.apache.spark.sql.DataFrame = [srcId: string, dstID: string]\n"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "val df_karate_splitted = df_karate.withColumn(\"_tmp\", split($\"value\", \"\\\\ \")).select(\n",
    "  $\"_tmp\".getItem(0).as(\"srcId\"),\n",
    "  $\"_tmp\".getItem(1).as(\"dstID\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[19] at rdd at <console>:39\n"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "val rows: RDD[Row] = df_karate_splitted.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "a: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[20] at map at <console>:39\n",
       "default_user: (Int, Int) = (0,0)\n",
       "graph: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@20f7a8df\n"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "var a= rows.map{ case Row(src:String, dist : String) => Edge(src.toLong, dist.toLong,1)}\n",
    "var default_user=(0, 0)\n",
    "var graph = Graph.fromEdges(a, default_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,1), Edge(3,1,1), Edge(3,2,1), Edge(4,1,1), Edge(4,2,1), Edge(4,3,1), Edge(5,1,1), Edge(6,1,1), Edge(7,1,1), Edge(7,5,1), Edge(7,6,1), Edge(8,1,1), Edge(8,2,1), Edge(8,3,1), Edge(8,4,1), Edge(9,1,1), Edge(9,3,1), Edge(10,3,1), Edge(11,1,1), Edge(11,5,1), Edge(11,6,1), Edge(12,1,1), Edge(13,1,1), Edge(13,4,1), Edge(14,1,1), Edge(14,2,1), Edge(14,3,1), Edge(14,4,1), Edge(17,6,1), Edge(17,7,1), Edge(18,1,1), Edge(18,2,1), Edge(20,1,1), Edge(20,2,1), Edge(22,1,1), Edge(22,2,1), Edge(26,24,1), Edge(26,25,1), Edge(28,3,1), Edge(28,24,1), Edge(28,25,1), Edge(29,3,1), Edge(30,24,1), Edge(30,27,1), Edge(31,2,1), Edge(31,9,1), Edge(32,1,1), Edge(32,25,1), Edge(32,26,1), Edge(32,29,1), Edge(33,3,1), Edge(33,9,1), Edge(33,15,1), Edge(33...\n"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "graph.edges.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res2: Array[(org.apache.spark.graphx.VertexId, (Int, Int))] = Array((20,(0,0)), (13,(0,0)), (19,(0,0)), (34,(0,0)), (15,(0,0)), (4,(0,0)), (21,(0,0)), (16,(0,0)), (22,(0,0)), (25,(0,0)), (28,(0,0)), (29,(0,0)), (11,(0,0)), (14,(0,0)), (32,(0,0)), (30,(0,0)), (24,(0,0)), (27,(0,0)), (33,(0,0)), (23,(0,0)), (1,(0,0)), (6,(0,0)), (17,(0,0)), (3,(0,0)), (7,(0,0)), (9,(0,0)), (8,(0,0)), (12,(0,0)), (18,(0,0)), (31,(0,0)), (26,(0,0)), (10,(0,0)), (5,(0,0)), (2,(0,0)))\n"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "graph.vertices.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "newgraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@abba59d\n"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "val newgraph = graph.outerJoinVertices(graph.degrees)((index,_,deg) => (index,deg.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.graphx.EdgeTriplet[(org.apache.spark.graphx.VertexId, Int),Int]] = Array(((2,(2,9)),(1,(1,16)),1), ((3,(3,10)),(1,(1,16)),1), ((3,(3,10)),(2,(2,9)),1), ((4,(4,6)),(1,(1,16)),1), ((4,(4,6)),(2,(2,9)),1), ((4,(4,6)),(3,(3,10)),1), ((5,(5,3)),(1,(1,16)),1), ((6,(6,4)),(1,(1,16)),1), ((7,(7,4)),(1,(1,16)),1), ((7,(7,4)),(5,(5,3)),1), ((7,(7,4)),(6,(6,4)),1), ((8,(8,4)),(1,(1,16)),1), ((8,(8,4)),(2,(2,9)),1), ((8,(8,4)),(3,(3,10)),1), ((8,(8,4)),(4,(4,6)),1), ((9,(9,5)),(1,(1,16)),1), ((9,(9,5)),(3,(3,10)),1), ((10,(10,2)),(3,(3,10)),1), ((11,(11,3)),(1,(1,16)),1), ((11,(11,3)),(5,(5,3)),1), ((11,(11,3)),(6,(6,4)),1), ((12,(12,1)),(1,(1,16)),1), ((13,(13,2)),(1,(1,16)),1), ((13,(13,2)),(4,(4,6)),1), ((14,(14,5)),(1,(1,16)),1), ((14,(14,5)),(2,(2,9)),1), ((14,(14,...\n"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "newgraph.triplets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res5: Array[(org.apache.spark.graphx.VertexId, (org.apache.spark.graphx.VertexId, Int))] = Array((20,(20,3)), (13,(13,2)), (19,(19,2)), (34,(34,17)), (15,(15,2)), (4,(4,6)), (21,(21,2)), (16,(16,2)), (22,(22,2)), (25,(25,3)), (28,(28,4)), (29,(29,3)), (11,(11,3)), (14,(14,5)), (32,(32,6)), (30,(30,4)), (24,(24,5)), (27,(27,2)), (33,(33,12)), (23,(23,2)), (1,(1,16)), (6,(6,4)), (17,(17,2)), (3,(3,10)), (7,(7,4)), (9,(9,5)), (8,(8,4)), (12,(12,1)), (18,(18,2)), (31,(31,4)), (26,(26,3)), (10,(10,2)), (5,(5,3)), (2,(2,9)))\n"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "newgraph.vertices.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sigmaIn2: (comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "kiin2: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "sigmaTot2: (comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "def sigmaIn2(comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcAttr._1==comId && triplet.dstAttr._1==comId))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin2(i:Long, comId : Long, graph : Graph[(Long, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstAttr._1==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def sigmaTot2(comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sigmaIn1: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "sigmaTot1: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "def sigmaIn1(i:Long, comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1==comId) &&\n",
    "                                                    !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def sigmaTot1(i:Long, comId: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==comId && triplet.dstAttr._1!=comId)||(triplet.srcAttr._1!=comId && triplet.dstAttr._1==comId) && !(triplet.srcId==i || triplet.dstId==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ki: (i: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n",
       "kiin: (i: Long, comId: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "def ki(i: Long, graph : Graph[(Long, Int),Int]): Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => triplet.srcId==i || triplet.dstId==i))\n",
    "    return graphVar.count.toDouble\n",
    "}\n",
    "\n",
    "def kiin(i:Long, comId : Long, graph : Graph[(Long, Int),Int]) : Double = {\n",
    "    var graphVar = graph.triplets.filter((triplet => (triplet.srcAttr._1==i && triplet.dstAttr._1==comId) ||(triplet.srcAttr._1==comId && triplet.dstAttr._1==i)))\n",
    "    return graphVar.count.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DQ: (sigmain: Double, sigmatot: Double, kivar: Double, kiinvar: Double, m: Long, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])Double\n"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "def DQ(sigmain:Double, sigmatot:Double, kivar:Double, kiinvar:Double, m:Long, graph:Graph[(Long, Int),Int]):Double = {\n",
    "    return (((sigmain+kiinvar)/(2*m) - scala.math.pow((sigmatot+kivar)/(2*m),2)) - (sigmain/(2*m) - scala.math.pow(sigmatot/(2*m),2) - scala.math.pow(kivar/(2*m),2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sigmain1: Double = 0.0\n",
       "sigmatot1: Double = 5.0\n",
       "kivar: Double = 16.0\n",
       "kiinvar: Double = 1.0\n",
       "m: Long = 39\n"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "var sigmain1 = sigmaIn1(1,14,newgraph)\n",
    "var sigmatot1 = sigmaTot1(1,14,newgraph)\n",
    "var kivar = ki(1, newgraph)\n",
    "var kiinvar = kiin2(1,14,newgraph)\n",
    "var m=newgraph.edges.count()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dq1: Double = -0.013477975016436564\n"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "var dq1 = DQ(sigmain1, sigmatot1, kivar, kiinvar, m , newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "a: Int = 0\n",
       "setDq: Set[(Int, Double)] = Set((4,-0.018737672583826435), (16,0.002301117685733052), (11,-0.002958579881656806), (8,-0.008218277449046671), (34,-0.07659434582511504), (30,-0.008218277449046671), (22,0.002301117685733052), (17,0.002301117685733052), (5,-0.002958579881656806), (26,-0.002958579881656806), (20,-0.002958579881656806), (23,0.002301117685733052), (33,-0.05029585798816569), (6,-0.008218277449046671), (27,0.002301117685733052), (2,-0.03451676528599608), (25,-0.002958579881656806), (18,0.002301117685733052), (3,-0.03977646285338594), (9,-0.013477975016436564), (19,0.002301117685733052), (28,-0.008218277449046671), (21,0.002301117685733052), (7,-0.008218277449046671), (14,-0.013477975016436564), (31,-0.008218277449046671), (29,-0.002958579881656806), (15,0.002301117685...\n"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "var a = 0\n",
    "var setDq =Set(): Set[(Int,Double)]\n",
    "for (a <- 2 to 34){\n",
    "    var sigmaindist = sigmaIn2(a.toLong, newgraph)\n",
    "    var sigmatotdist = sigmaTot2(a.toLong, newgraph)\n",
    "    var dq2 = DQ(sigmaindist, sigmatotdist, kivar, kiinvar, m, newgraph)\n",
    "    setDq+=((a,dq2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "maxCom: (setDq: Set[(Int, Double)])Int\n"
      ]
     },
     "metadata": {},
     "execution_count": 224
    }
   ],
   "source": [
    "def maxCom(setDq : Set[(Int, Double)]) : Int = {\n",
    "    var max = -1000.0\n",
    "    var ind = -1\n",
    "    setDq.foreach(x => {\n",
    "        if (x._2>max)\n",
    "        {\n",
    "            ind=x._1\n",
    "            max=x._2\n",
    "        }\n",
    "    })\n",
    "    return ind\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res63: Int = 12\n"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "maxCom(setDq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "mooveItoC: (i: Int, comId: Int, graph: org.apache.spark.graphx.Graph[(Long, Int),Int])org.apache.spark.graphx.Graph[(Long, Int),Int]\n"
      ]
     },
     "metadata": {},
     "execution_count": 299
    }
   ],
   "source": [
    "def mooveItoC(i:Int, comId : Int, graph:Graph[(Long, Int),Int]) : Graph[(Long, Int),Int] = {\n",
    "    val newVertices = newgraph.mapVertices { case (id, attr) => if (id==i) (comId.toLong, attr._2) else attr }\n",
    "    return newVertices\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}